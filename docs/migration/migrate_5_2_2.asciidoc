[[migration]]
== Migrating from Kibi 4.x

[float]
=== Upgrading Elasticsearch

Kibi is compatible with Elasticsearch {elasticsearch-version}; this section
provides a summary of the upgrade procedure described at
{elastic-ref}/setup-upgrade.html[Upgrading Elasticsearch] with Kibi and Search
Guard specific notes.

It is also recommended to read the
{elastic-ref}/breaking-changes-5.0.html[Breaking Changes in 5.0] section of the
Elasticearch documentation.

[float]
==== Pre-requisites

It is recommended to have a backup of the indices in the cluster;
if running a production cluster, you will need to request an updated license
for Siren Federate.

[float]
===== Search Guard configuration backup

If your cluster is secured by Search Guard, you will need to retrieve
the current cluster configuration through the `sgadmin` tool;
after the upgrade, you'll need to change these files with new
permission identifiers and reload it to the cluster.

`sgadmin.sh` is available in the _plugins/search-guard-5/tools_ directory on
each Elasticsearch instance in which Search Guard has been installed; a
standalone version (`sgadmin-standalone.zip`) can be downloaded from
{searchguard-sgadmin-ref}[this page].

The current configuration can be retrieved by creating a backup directory
and running the tool as follows:

[source,bash]
----
mkdir configbak
bash /path/to/sgadmin.sh -r -ks CN\=sgadmin-keystore.jks -cd configbak -kspass password -ts truststore.jks -tspass password -icl -nhnv -h elasticsearch.local -p 9300
----

Arguments:

- `r`: retrieve configuration.
- `cd`: the directory in which the current configuration will be saved.
- `ks`: the path to the administrative keystore
- `kspass`: the password of the administrative keystore.
- `ts`: the path to the truststore.
- `tspass`: the password of the truststore.
- `icl`: ignore cluster name.
- `nhvv`: do not verify cluster hostname.
- `h`: Elasticsearch hostname.
- `p`: Elasticsearch transport port.

If execution is successful, you will find the five Search Guard configuration
files in the `configbak` directory suffixed by the current date.

Before proceeding, **ensure that the files are not empty** and remove the date
suffix; you should end up with the following files which will be required later.

- `sg_action_groups.yml`
- `sg_config.yml`
- `sg_internal_users.yml`
- `sg_roles_mapping.yml`
- `sg_roles.yml`

[float]
==== Upgrade preparation

To upgrade from Elasticsearch 2.4.x, you will need to perform a full cluster
restart.

Before proceeding, disable shard allocation to avoid unnecessary traffic during
the shutdown of each node; this can be done by sending the following request
with curl:

[source,bash]
----
curl -XPUT http://elasticsearch.local:9200/_cluster/settings -d '{
  "persistent": {
    "cluster.routing.allocation.enable": "none"
  }
}'
----

If your cluster is secured by Search Guard, remember to include your
credentials in each request, e.g.:

[source,bash]
----
curl -uadmin:password --cacert ca.pem -XPUT https://elasticsearch.local:9200/_cluster/settings -d '{
  "persistent": {
    "cluster.routing.allocation.enable": "none"
  }
}'
----

For a faster recovery, you can optionally stop any indexing activity and issue
a {elastic-ref}indices-synced-flush.html[synced flush] request as follows:

[source,bash]
----
curl -XPOST http://elasticsearch.local:9200/_flush/synced
----

[float]
===== Node upgrade

Once the previous steps have been carried out, you will need to stop all the
nodes in the cluster and upgrade Elasticsearch and plugins on each node as
explained in the following sections.

**Do not restart nodes until the software has been upgraded on the whole
cluster.**

[float]
====== Elasticsearch upgrade

If using Elastic packages, you can upgrade Elasticsearch by installing the
compatible `rpm` or `deb` package through `yum` or `apt`; the packages should
preserve the existing configuration files and data directories.

If Elasticsearch was installed from a zip or tar distribution, you will need
to:

- download a compatible version in zip or tar format from
  https://www.elastic.co/downloads/elasticsearch .
- extract the package to a new location; make sure to not overwrite the
  existing installation when extracting the archive.
- copy all files from the `config` directory of the previous installation to the `config`
  directory of the new installation.
- check the value of `path.data` in `config/elasticsearch.yml`; if not set, you
  will need to copy the `data` directory from the old installation to the new
  one, otherwise Elasticsearch will keep using the data directory specified in
  `path.data`.
- ensure that the new location has the correct filesystem permissions for your
  environment (usually it is owned by the user running Elasticsearch).

*Configuration changes*

Any index level setting (settings starting with `index`) declared in
`elasticsearch.yml` must be removed from the file; these settings have now to
be declared in {elastic-ref}indices-templates.html[Index templates].

The following settings must be removed if present:

- `index.queries.cache.everything`
- `siren.filterjoin.cache.size`

*Startup scripts*

If you were starting Elasticsearch using custom systemd/init.d/supervisor
scripts, make sure to check the
{elastic-ref}breaking_50_packaging.html[packaging breaking changes] section of
the Elasticsearch documentation as several environment variables have now to be
set in the `jvm.options` file.

It is also recommended to verify your custom scripts and system properties
against the updated {elastic-ref}system-config.html[Important System
Configuration] documentation.

[float]
====== Plugins upgrade

If you installed Elasticsearch from system packages, you will need to remove
the old versions of your plugins `/usr/share/elasticsearch/plugins`; old
plugins can be removed by using the `elasticsearch-plugin` command, e.g.:

[source,bash]
----
/usr/share/elasticsearch/bin/elasticsearch-plugin remove siren-join
----

If installed, the following plugins must be removed:

- `siren-join`
- `license-siren`
- `kopf`
- `search-guard-2`
- `search-guard-ssl`

[float]
====== Siren Federate

The Siren Federate plugin for Elasticsearch replaces the Siren Join and Siren
License plugins; to install the plugin, you need to download the zip package of
the plugin from {download-ref}[our customer portal] and install it through
`elasticsearch-plugin`, e.g.:

[source,bash]
----
./usr/share/elasticsearch/bin/elasticsearch-plugin install file:///path/to/siren-federate-<version>-plugin.zip
----

[float]
====== Search Guard 5

If your cluster is protected by Search Guard, you will need to replace the
Search Guard 2 and Search Guard SSL plugins with the Search Guard 5 plugin;
to find out the version of Search Guard 5 compatible with your
Elasticsearch installation, please check the {searchguard-matrix-ref}[Search
Guard version matrix].

The plugin can be installed from Maven Central using `elasticsearch-plugin`:

[source,bash]
----
./usr/share/elasticsearch/bin/elasticsearch-plugin install -b com.floragunn:search-guard-5:<version>
----

In addition, the compatible version of any commercial Search Guard add-on will
have to be downloaded and copied to the `plugins/search-guard-5` directory; the
following list provides links to the download page of all the add-ons commonly
used in Kibi instances:

- {searchguard-management-api-ref}[REST Management API] 
- {searchguard-dlsfls-ref}[Document/Field Level security module]
- {searchguard-ldap-ref}[LDAP Authentication module]
- {searchguard-kerberos-ref}[Kerberos Authentication module]
- {searchguard-jwt-ref}[JWT Authentication module]

[float]
===== Cluster restart without Search Guard

If your cluster is protected by Search Guard, skip to the next section for
specific instructions.

Once Elasticsearch and plugins are up to date on all nodes, you can begin
restarting nodes; it is recommended to restart master eligible nodes first and
wait for the cluster to elect a master before proceeding with other nodes.

NOTE: master eligibile nodes are nodes having `node.master` set to `true` or
not set in `elasticsearch.yml`.

Wait for cluster to reach the Yellow status; if not using Search Guard, you can
check the status of the cluster by issuing a requests with curl to
{elastic-ref}cat-health.html[_cat/health], e.g.:

[source,bash]
----
$ curl http://elasticsearch.local:9200/_cat/health?v
epoch      timestamp cluster status node.total node.data shards ...
1498542620 06:50:20  541     yellow          5         3    106 ...
----

Once the cluster is in the Yellow status, you can re-enable shard allocation:

[source,bash]
----
$ curl -XPUT http://elasticsearch.local:9200/_cluster/settings -d '
{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}
'
----

Then you can issue additional requests to `_cat/health` to monitor the cluster
until it reaches the Green status; you can also check the recovery status of
each shard by issuing requests to `_cat/recovery`, e.g.:

[source,bash]
----
$ curl http://elasticsearch.local:9200/_cat/recovery
----

[float]
===== Cluster restart with Search Guard

Once Elasticsearch and plugins are up to date on all nodes, you can begin
restarting nodes; it is recommended to restart master eligible nodes first and
wait for the cluster to elect a master before proceeding with other nodes.

NOTE: master eligibile nodes are nodes having `node.master` set to `true` or
not set in `elasticsearch.yml`.

You won't be able to issue HTTP requests until the Search Guard configuration
has been upgraded, so you will need to use `sgadmin` to re-enable shard
allocation after the cluster reaches the Yellow status.

`sgadmin.sh` is available in the _plugins/search-guard-5/tools_ directory on
each Elasticsearch instance in which Search Guard has been installed; a
standalone version (`sgadmin-standalone.zip`) can be downloaded from
{searchguard-sgadmin-ref}[this page].

To re-enable shard allocation with `sgadmin` execute it as follows:

[source,bash]
----
$ bash /path/to/sgadmin.sh -esa -ks CN=sgadmin-keystore.jks -kspass password -ts config/truststore.jks -tspass password -icl -nhnv -h elasticsearch.local -p 9300
Search Guard Admin v5
Will connect to elasticsearch.local:9330 ... done
Persistent and transient shard allocation enabled
----

Arguments:

- `esa`: enable shard allocation
- `ks`: the path to the administrative keystore
- `kspass`: the password of the administrative keystore.
- `ts`: the path to the truststore.
- `tspass`: the password of the truststore.
- `icl`: ignore cluster name.
- `nhvv`: do not verify cluster hostname.
- `h`: Elasticsearch hostname.
- `p`: Elasticsearch transport port.

Then you need to reload the Search Guard configuration retrieved before
upgrading the cluster by running sgadmin as follows:

[source,bash]
----
$ bash /path/to/sgadmin.sh -ks CN\=sgadmin-keystore.jks -cd configbak -kspass password -ts truststore.jks -tspass password -icl -nhnv -h elasticsearch.local -p 9300
Search Guard Admin v5
Will connect to elasticsearch.local:9330 ... done
Contacting elasticsearch cluster 'elasticsearch' and wait for YELLOW clusterstate ...
Clustername: escluster
Clusterstate: YELLOW
Number of nodes: 1
Number of data nodes: 1
searchguard index already exists, so we do not need to create one.
Will update 'config' with configbak/sg_config.yml
   SUCC: Configuration for 'config' created or updated
Will update 'roles' with configbak/sg_roles.yml
   SUCC: Configuration for 'roles' created or updated
Will update 'rolesmapping' with configbak/sg_roles_mapping.yml
   SUCC: Configuration for 'rolesmapping' created or updated
Will update 'internalusers' with configbak/sg_internal_users.yml
   SUCC: Configuration for 'internalusers' created or updated
Will update 'actiongroups' with configbak/sg_action_groups.yml
   SUCC: Configuration for 'actiongroups' created or updated
Done with success
----

Arguments:

- `cd`: the directory containing old Search Guard configuration backup.
- `ks`: the path to the administrative keystore
- `kspass`: the password of the administrative keystore.
- `ts`: the path to the truststore.
- `tspass`: the password of the truststore.
- `icl`: ignore cluster name.
- `nhvv`: do not verify cluster hostname.
- `h`: Elasticsearch hostname.
- `p`: Elasticsearch transport port.

You should now be able issue authenticated requests to `_cat/health` to monitor
the cluster until it reaches the Green status; you can also check the recovery
status of each shard by issuing requests to `_cat/recovery`, e.g.:

[source,bash]
----
$ curl -uadmin:password --cacert ca.pem https://elasticsearch.local:9200/_cat/health
$ curl -uadmin:password --cacert ca.pem https://elasticsearch.local:9200/_cat/recovery
----

Once the cluster has recovered, you will need to modify the Search Guard
configuration to adjust Kibi specific permissions as described in the next
section.

[float]
=== Search Guard configuration changes

This section describes all the Kibi specific changes required to the Search
Guard configuration files; it is advised to backup the files in the retrieved
configuration directory before making changes.

Once the files have been modified, the updated configuration can
be loaded to the cluster using `sgadmin`.

[float]
==== sg_action_groups.yml

Add the following action groups:

[source,yaml]
----
UNLIMITED:
  - "*"

INDICES_ALL:
  - "indices:*"
----

Set the `ALL` action group as follows:

[source,yaml]
----
ALL:
  - INDICES_ALL
----

Modify the `CREATE_INDEX` action group as follows:

[source,yaml]
----
CREATE_INDEX:
  - "indices:admin/create"
  - "indices:admin/mapping/put"
----

Add the `INDICES_MONITOR` action group:

[source,yaml]
----
MONITOR:
  - INDICES_MONITOR

INDICES_MONITOR:
  - "indices:monitor/*"
----

Update the `DATA_ACCESS`, `READ`, `WRITE`, `DELETE`, `CRUD` and `SEARCH` groups
as follows:

[source,yaml]
----
DATA_ACCESS:
  - "indices:data/*"
  - CRUD

WRITE:
  - "indices:data/write*"
  - "indices:admin/mapping/put"

READ:
  - "indices:data/read*"
  - "indices:admin/mappings/fields/get*"

DELETE:
  - "indices:data/write/delete*"

CRUD:
  - READ
  - WRITE

SEARCH:
  - "indices:data/read/search*"
  - "indices:data/read/msearch*"
  - "indices:siren/plan*"
  - "indices:siren/mplan*"
  - SUGGEST
----

Update the `INDEX` group as follows:

[source,yaml]
----
INDEX:
  - "indices:data/write/index*"
  - "indices:data/write/update*"
  - "indices:admin/mapping/put"
  - "indices:data/write/bulk*"

----

Add the `CLUSTER_COMPOSITE_OPS_RO` and `CLUSTER_COMPOSITE_OPS` roles:

[source,yaml]
----
CLUSTER_COMPOSITE_OPS_RO:
  - "indices:data/read/mget"
  - "indices:data/read/msearch"
  - "indices:siren/mplan"
  - "indices:data/read/mtv"
  - "indices:admin/aliases/exists*"
  - "indices:admin/aliases/get*"

CLUSTER_COMPOSITE_OPS:
  - "indices:data/write/bulk*"
  - "indices:admin/aliases*"
  - CLUSTER_COMPOSITE_OPS_RO
----

Remove the `KIBI_MSEARCH` role.

Add the `KIBI_COMPOSITE` role:

[source,yaml]
----
KIBI_COMPOSITE:
  - "indices:siren/mplan*"
----

Replace `KIBI_CLUSTER`, `KIBI_READONLY` and `KIBI_READWRITE` with the following
definitions:

[source,yaml]
----
KIBI_CLUSTER:
  - "indices:data/read/scroll"
  - "cluster:internal/data/transfer/*"
  - "indices:data/read/msearch*"
  - "indices:siren/mplan*"
  - "cluster:admin/plugin/siren/license/get"
  - CLUSTER_COMPOSITE_OPS_RO

KIBI_READONLY:
  - "indices:data/read/field_stats*"
  - "indices:data/read/field_caps*"
  - "indices:data/read/get*"
  - "indices:data/read/mget*"
  - "indices:data/read/search*"
  - "indices:siren/mplan"
  - "indices:siren/plan"
  - "indices:siren/task/search"
  - "indices:admin/mappings/get*"
  - "indices:admin/mappings/fields/get*"
  - "indices:admin/validate/query*"
  - "indices:admin/get*"
  - "indices:admin/version/get*"
  - KIBI_COMPOSITE

KIBI_READWRITE:
  - "indices:admin/exists*"
  - "indices:admin/mapping/put*"
  - "indices:admin/refresh*"
  - "indices:data/write/delete*"
  - "indices:data/write/index*"
  - "indices:data/write/update*"
  - "indices:data/write/bulk*"
  - KIBI_READONLY
----

If you defined additional action groups, you will need to verify that the
permissions work as expected in Search Guard 5.

[float]
==== sg_roles.yml

[float]
===== kibiserver

Replace the `kibiserver` role with the following; **make sure to set the
correct names for the main kibi (`.kibi` by default) and access control
(`.kibiaccess` by default) indices if using a custom configuration**.

[source,yaml]
----
# Permissions for the Kibi server process.
kibiserver:
  cluster:
      - cluster:monitor/nodes/info
      - cluster:monitor/health
      - cluster:monitor/main
      - cluster:monitor/state
      - cluster:monitor/nodes/stats
      - KIBI_CLUSTER
      - CLUSTER_COMPOSITE_OPS
  indices:
    '*':
      '*':
        - indices:admin/get
    '?kibi':
      '*':
        - ALL
    '?kibiaccess':
      '*':
        - ALL
----

[float]
===== sentinl

Sentinl is automatically enabled in Kibi 5; if you do not need alerting it
is possible to disable the plugin by putting the following in `kibi.yml`:

[source,yaml]
----
sentinl:
  enabled:
    false
----

If you were previously using Sentinl, replace the `sentinl` role with the
following; **make sure to set the correct names for watcher and watcher_alarms
indices if using a custom configuration**.

Replace the `sentinl` role with the following; **make sure to set the correct
names for watcher and watcher_alarms indices if using a custom configuration**.

[source,yaml]
----
# Permissions for a Sentinl user.
sentinl:
  cluster:
    - "indices:data/write/bulk*"
    - "indices:admin/template/*"
    - KIBI_CLUSTER
  indices:
    'watcher_alarms*':
      '*':
        - KIBI_READWRITE
        - CREATE_INDEX
    '/(watcher|watcher_alarms)/':
      '*':
        - KIBI_READWRITE
        - CREATE_INDEX
----

You will also need to grant read permissions to each data index that Sentinl
should be able to access when executing watches; for example, to grant access
to the index `readings` and all indices starting with `logs`, the role would
become:

[source,yaml]
----
# Permissions for a Sentinl user.
sentinl:
  cluster:
    - "indices:data/write/bulk*"
    - "indices:admin/template/*"
    - KIBI_CLUSTER
  indices:
    'watcher_alarms*':
      '*':
        - KIBI_READWRITE
        - CREATE_INDEX
    '/(watcher|watcher_alarms)/':
      '*':
        - KIBI_READWRITE
        - CREATE_INDEX
    'readings':
      '*':
        - KIBI_READONLY
    'logs*':
      '*':
        - KIBI_READONLY
----


[float]
===== Kibi user roles

In each Kibi user role:
 
- replace `KIBI_MSEARCH` with `KIBI_COMPOSITE`.
- remove the following permission on the Kibi index (`.kibi` by default) from
  each user role that has it:

[source,yaml]
----
    ?kibi:
      null:
        - "indices:data/read/search"
        - "indices:data/read/coordinate-search"
----

If you are using the Kibi access control plugin and want to enable ACLs on Kibi
saved objects (which restricts access to saved objects), ensure that Kibi users
**do not** have access to the Kibi index (`.kibi` by default), otherwise they
might be able to peek at saved objects by issuing custom Elasticsearch queries.

Access to the `.kibi` index was usually granted by the following lines in
previous Kibi releases; these can be removed when using ACLs as the .kibi index
will be accessed only by the `kibiserver` role:

```
'?kibi':
  '*':
    - KIBI_READWRITE
```

```
'?kibi':
  '*':
    - KIBI_READONLY
```

If there are users with read access to all indices (`*`), they will be able to
see the Kibi index as well; if you need to hide it, you should replace the
wildcard with a list of rules for each index.

*Monitoring*

To grant a user the permission to use monitoring plugins and retrieve
diagnostics information from the UI, you will need to add the `CLUSTER_MONITOR`
and `INDICES_MONITOR` permissions to its role, e.g.:

[source,yaml]
----
# Permissions for a Kibi administrator (read-write access to the .kibi index).
kibiadmin:
  cluster:
    - KIBI_CLUSTER
    - cluster:admin/plugin/siren/license/put
    - CLUSTER_MONITOR
  indices:
    '*':
      '*':
        - KIBI_READONLY
        - INDICES_MONITOR
    '?kibi':
      '*':
        - KIBI_READWRITE
    'watcher':
      '*':
        - KIBI_READWRITE
----

[float]
===== marvel / X-Pack monitoring

If you were using Marvel and are migrating to X-Pack monitoring, the following
role can be used in place of the previous sample `marvel` role:

[source,yaml]
----
# Permissions for an X-Pack monitoring agent.
monitoring:
  cluster:
    - CLUSTER_MONITOR
    - 'indices:admin/aliases'
    - 'indices:admin/template/get'
    - 'indices:admin/template/put'
    - 'cluster:admin/ingest/pipeline/get'
    - 'cluster:admin/ingest/pipeline/put'
    - 'indices:data/write/bulk'
  indices:
    '?marvel*':
      '*':
        - ALL
    '?monitoring*':
      '*':
        - ALL
----

[float]
==== Loading the modified configuration

To load the modified configuration run sgadmin as follows:

[source,bash]
----
$ bash /path/to/sgadmin.sh -ks CN\=sgadmin-keystore.jks -cd confignew -kspass password -ts truststore.jks -tspass password -icl -nhnv -h elasticsearch.local -p 9300
Search Guard Admin v5
Will connect to elasticsearch.local:9330 ... done
Contacting elasticsearch cluster 'elasticsearch' and wait for YELLOW clusterstate ...
Clustername: escluster
Clusterstate: YELLOW
Number of nodes: 1
Number of data nodes: 1
searchguard index already exists, so we do not need to create one.
Will update 'config' with confignew/sg_config.yml
   SUCC: Configuration for 'config' created or updated
Will update 'roles' with confignew/sg_roles.yml
   SUCC: Configuration for 'roles' created or updated
Will update 'rolesmapping' with confignew/sg_roles_mapping.yml
   SUCC: Configuration for 'rolesmapping' created or updated
Will update 'internalusers' with confignew/sg_internal_users.yml
   SUCC: Configuration for 'internalusers' created or updated
Will update 'actiongroups' with confignew/sg_action_groups.yml
   SUCC: Configuration for 'actiongroups' created or updated
Done with success
----

Arguments:

- `cd`: the directory containing the modified Search Guard configuration.
- `ks`: the path to the administrative keystore
- `kspass`: the password of the administrative keystore.
- `ts`: the path to the truststore.
- `tspass`: the password of the truststore.
- `icl`: ignore cluster name.
- `nhvv`: do not verify cluster hostname.
- `h`: Elasticsearch hostname.
- `p`: Elasticsearch transport port.

[float]
=== License

You will need to upload your license once the cluster is up and running.

==== In Kibi
To upload the license in Kibi, navigate to the management section.

image::images/management_no-license.jpg[]

This page will show the license has not been installed.

Click on `License` and select `Upload Licence`
 
image::images/license_page_before_install.jpg[] 

Once you have selected your license and it has been uploaded, the page will show the license details.

image::images/license_page_after_install.jpg[]

==== Via CLI
To upload the license you can use curl as follows:

[source,bash]
----
curl http://localhost:9220/_siren/license -XPUT -T license.sig
----

If your cluster is protected by Search Guard, remember to specify credentials:

[source,bash]
----
curl -uadmin:password --cacert=ca.pem https://localhost:9220/_siren/license -XPUT -T license.sig
----

[float]
=== Kibi

Before upgrading Kibi, make sure to perform the following steps:
 
- backup the `.kibi` index.
- backup the Kibi configuration file (`config/kibi.yml`)

Then perform the following steps:

- extract the new Kibi version to a new directory.
- copy the previous configuration file to the `config` directory of the new
  installation.
- copy the files from the `data` directory in your old installation of the new
  installation.
- copy the files from the `pki` directory in your old installation of the new
  installation.
- update the configuration as described in the next sections.
- reinstall third party plugins after checking the notes in the next sections.
- execute `bin/kibi upgrade`.
- start the new installation.

[float]
==== Configuration changes

[float]
===== Server SSL support

If SSL support was enabled in the previous Kibi installation, you will need to
update `kibi.yml` as follows:

- add `server.ssl.enabled: true` 
- replace `server.ssl.cert` with `server.ssl.certificate`
- replace `ssl.verify` with `ssl.verificationMode`; `ssl.verificationMode` can be
  set to `full`, to check both certificate and hostname, `certificate` to check
  only the certificate, `none` to disable verification.
  If not set, the verification mode defaults to `full`.

[float]
===== Search Guard client CA

If your cluster is protected by Search Guard, you will need to rename
`elasticsearch.ssl.ca` to `elasticsearch.ssl.certificateAuthorities`.


[float]
===== Kibi access control ACL

If you want to enable Kibi Access Control ACL, you will need to:

- add `acl.enabled: true` to the `kibi_access_control` section;
- specify the Search Guard role that will have administrative access to the ACL configuration
  in `admin_role` (`.kibiadmin` by default);
- specify the name of the index where access control rules will be stored (`.kibiaccess` by default).

E.g.:

[source,yaml]
----
kibi_access_control:
  acl:
    enabled: true
    index: kibiaccess
  admin_role: kibiadmin
----

Once ACLs are enabled, you will need to login with a user having the role specified in
`admin_role` and visit Access Control -> ACL to create Kibi roles with the desired
permissions.

For more informations about Kibi Access Control ACL, please check the
<<searchguard_integration>> chapter.

[float]
===== Sense

`sense` settings must now be declared in the `console` section, e.g.:

[source,yaml]
----
console.proxyConfig:
  - match:
      protocol: "https"

    ssl:
      ca: "pki/searchguard/ca.pem"
----


[float]
==== Third party plugins

The following plugins have been discontinued:

- `kibi_wordcloud`: Kibi now supports the Tag Cloud visualization introduced in
  Kibana 5. Existing Kibi Word Cloud visualizations will be migrated to Tag
  Cloud visualizations by the upgrade procedure.
- `heatmap`: the heatmap plugin for Kibana 4.x has been integrated into Kibana
  5 and thus is available in Kibi.  Existing Heatmap visualizations will be
  upgraded by a Kibi migration; the color palette will be set to `Greens` as
  most of the palettes in the original plugin were not ported to Kibana 5.

[float]
==== Handling of text fields in upgraded indices

Fields which were declared in mappings with `type` set to `string` and `index`
to `analyzed` (which was the default before Elasticsearch 5.0) will be handled
as fields of type {elastic-ref}/fielddata.html[text].

In order to be usable for aggregation and sorting operations, fields of type
`text` need a in-memory structure called `fielddata`, which is disabled by
default to reduce memory usage.

If you have visualizations using `text` fields in aggregations (for example tag
clouds), you will need to enable `fielddata` in their mapping; for example, the
following command sets `fielddata` to `true` on the `snippet` field of an
`article` type in an index called `article`:

[source,bash]
----
curl -XPUT http://elasticsearch:9200/article/_mapping/article - d'
{
  "properties": {
    "snippet": { 
      "type": "text",
      "fielddata": true
    }
  }
}
'
----

If the existing mapping defines custom parameters (e.g. `analyzer`) you will
get a conflicting mapping error; in that case, you need to include the custom
parameters in the updated mapping definition, e.g.:

[source,bash]
----
curl -XPUT http://elasticsearch.local:9200/article/_mapping/article - d'
{
  "properties": {
    "snippet": { 
      "type": "text",
      "analyzer": "english",
      "fielddata": true
    }
  }
}
'
----

After enabling fielddata on an existing text field, you'll have to refresh all
the index patterns containing the field by going to Management/Index Patterns
in Kibi, opening each pattern and clicking on the refresh button; once the
index pattern is refreshed, the field should appear as aggregatable.

image::images/refresh.png["Refreshing an index pattern",align="center"]

[float]
===== Additional documentation

For more information please refer to the following links:

- {elastic-ref}/fielddata.html[text datatype documentation]
- {elastic-ref}/fielddata.html[fielddata documentation]

[[breaking-changes-5.4.0]]
== Breaking changes in Kibi and Kibana 5.4.0

This section list the changes in Kibi and Kibana that you need to be aware of
when migrating to Kibi 5.4.0.

[float]
=== Kibana binds to localhost by default
{pull}8013[Pull Request 8013]

*Details:* Kibana (like Elasticsearch) now binds to localhost for security purposes instead of 0.0.0.0 (all addresses). Previous binding to 0.0.0.0 also caused issues for Windows users.

*Impact:* If you are running Kibana inside a container/environment that does not allow localhost binding, this will cause Kibana not to start up unless server.host is configured in the kibana.yml to a valid IP address/host, etc..

[float]
=== Markdown headers

{pull}7855[Pull Request 7855]

*Details:* As part of addressing the security issue https://www.elastic.co/community/security[ESA-2016-03] (CVE-2016-1000220) in the Kibana product, the markdown version has been bumped.

*Impact:* As a result of the fix to ESA-2016-03, there is a slight change in the markdown format for headers.

Previously, headers are defined using `###` followed by the title:

 ###Packetbeat:
    [Dashboard](/#/dashboard/Packetbeat-Dashboard)
    [Web transactions](/#/dashboard/HTTP)

It should now be defined as follows (with a space between ### and the title):

 ### Packetbeat:
     [Dashboard](/#/dashboard/Packetbeat-Dashboard)
     [Web transactions](/#/dashboard/HTTP)

[float]
=== Only whitelisted client headers are sent to Elasticsearch

{pull}6896[Pull Request 6896]

*Details:* The only headers that are proxied from the browser client to Elasticsearch are the ones set via the `elasticsearch.requestHeadersWhitelist` server configuration.

*Impact:* If you're relying on client headers in Elasticsearch, you will need to whitelist the specific headers in your `kibana.yml`.

[float]
=== `server.defaultRoute` is now always prefixed by `server.basePath`

{pull}6953[Pull Request 6953]

*Details:* The base path configuration now precedes the default route configuration when accessing the default route.

*Impact:* If you were relying on both `defaultRoute` and `basePath` configurations, you will need to remove the hardcoded `basePath` from your `defaultRoute`.

[float]
=== Directory listings of static assets are no longer rendered

{pull}6764[Pull Request 6764]

*Details:* The server no longer renders a list of static files if you try to access a directory.

*Impact:* If you were relying on this behavior before, you will need to expose underlying directory listings via a reverse proxy instead.

[float]
=== Console logs display date/time in UTC

{pull}8534[Pull Request 8534]

*Details:* All server logs now render in UTC rather than the server's local time.

*Impact:* If you are parsing the timestamps of Kibana server logs in an automated way, make sure to update your automation to accomodate UTC values.

[float]
=== A column for Average no longer renders along with Standard Deviation

{pull}7827[Pull Request 7827]

*Details:* From the early days of Kibana, adding a standard deviation metric to a data table also resulted in an average column being added to that data table. This is no longer the case.

*Impact:* If you want to have both standard deviation and average in the same data table, then add both columns just as you would any other metric.

[float]
=== Minimum size on terms aggregations has been changed from 0 to 1

{pull}8339[Pull Request 8339]

*Details:* Elasticsearch has removed the ability to specify a size of 0 for terms aggregations, so Kibana's minimum value has been adjusted to follow suit.

*Impact:* Any saved visualization that relies on size=0 will need to be updated.

[float]
=== Saved objects with previously deprecated Elasticsearch features

*Details:* Since Kibana 4.3, users have been able to arbitrarily modify filters
via a generic JSON editor. If users took advantage of any deprecated Elasticsearch
features in this way, then they will cause errors in Kibana since they're removed
from Elasticsearch 5.0. Check the Elasticsearch
{es-ref}/breaking_50_search_changes.html#_deprecated_queries_removed[breaking changes]
documentation for more details.

*Impact*: Discover, Visualize, and Dashboard will error for any saved objects that
are relying on removed Elasticsearch functionality. Users will need to update the
JSON of any affected filters.
